{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80bc4264-740e-4b8c-a0c1-3d70cdca2c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58e172ef-a8b7-4f92-9613-75b4c862eee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.1+cu117\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3daea3e-e40e-4389-bfe1-1d4c6b903c0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.7\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.version.cuda)  # CUDA version PyTorch was built with\n",
    "print(torch.cuda.is_available())  # Check if CUDA is available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d123131-ed58-4cf0-a5d2-646219235a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import torch\n",
    "from torch.utils.data import Subset, DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from models import DiT  # Assuming models.py contains the DiT implementation\n",
    "from diffusion import create_diffusion\n",
    "# from torch.nn.parallel import DistributedDataParallel as DDP # can only be used when multiple GPUs are available\n",
    "from copy import deepcopy\n",
    "from collections import OrderedDict\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 4 # 16 ?\n",
    "epochs = 5\n",
    "learning_rate = 1e-4\n",
    "\n",
    "# Custom transformation to add an extra channel\n",
    "class AddAlphaChannel(object):\n",
    "    def __call__(self, img):\n",
    "        # Add a new channel (e.g., copy of the red channel or noise)\n",
    "        img = torch.cat([img, img[0:1, :, :]], dim=0)  # Adding a copy of the first channel (RGB -> RGBA)\n",
    "        return img\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((32, 32)),\n",
    "    transforms.ToTensor(),\n",
    "    #AddAlphaChannel(),  # Add the extra channel\n",
    "    #transforms.Normalize((0.5, 0.5, 0.5, 0.5), (0.5, 0.5, 0.5, 0.5))  # Normalize 4 channels\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize 3 channels\n",
    "])\n",
    "\n",
    "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "533d69b5-982e-49b9-81f4-c026a6499140",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_size = int(len(train_dataset) * 0.25)\n",
    "\n",
    "# Generate a list of random indices\n",
    "indices = torch.randperm(len(train_dataset)).tolist()\n",
    "\n",
    "# Select the first 25% of the indices\n",
    "subset_indices = indices[:subset_size]\n",
    "\n",
    "# Create a new subset of the dataset\n",
    "subset_dataset = Subset(train_dataset, subset_indices)\n",
    "\n",
    "# Create a DataLoader for the subset\n",
    "train_loader = DataLoader(subset_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97c3394f-fd67-44f7-8ce7-83f8e570cb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3, 32, 32])\n",
      "tensor([4, 7, 4, 6])\n",
      "torch.Size([4, 3, 32, 32])\n",
      "tensor([5, 8, 6, 8])\n",
      "torch.Size([4, 3, 32, 32])\n",
      "tensor([4, 6, 1, 5])\n",
      "torch.Size([4, 3, 32, 32])\n",
      "tensor([1, 9, 2, 2])\n",
      "torch.Size([4, 3, 32, 32])\n",
      "tensor([2, 2, 3, 2])\n",
      "torch.Size([4, 3, 32, 32])\n",
      "tensor([2, 4, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "for i, (images, labels) in enumerate(train_loader):\n",
    "    print(images.shape)\n",
    "    print(labels)\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f095eb8-744c-4ed7-a937-fb19c7bae930",
   "metadata": {},
   "source": [
    "These labels are a 1 x B tensor where B is the batch size. Each label is an integer that ranges from 0 to 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8461ba8f-e7d2-4830-8366-221dd683c5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3125\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader)) # 3125 batches of size 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b955cfa-a862-4d0c-860b-97faa6bfc5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def update_ema(ema_model, model, decay=0.9999):\n",
    "    \"\"\"\n",
    "    Step the EMA model towards the current model.\n",
    "    \"\"\"\n",
    "    ema_params = OrderedDict(ema_model.named_parameters())\n",
    "    model_params = OrderedDict(model.named_parameters())\n",
    "\n",
    "    for name, param in model_params.items():\n",
    "        # TODO: Consider applying only to params that require_grad to avoid small numerical changes of pos_embed\n",
    "        ema_params[name].mul_(decay).add_(param.data, alpha=1 - decay)\n",
    "\n",
    "\n",
    "def requires_grad(model, flag=True):\n",
    "    \"\"\"\n",
    "    Set requires_grad flag for all parameters in a model.\n",
    "    \"\"\"\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = flag\n",
    "\n",
    "\n",
    "def cleanup():\n",
    "    \"\"\"\n",
    "    End DDP training.\n",
    "    \"\"\"\n",
    "    dist.destroy_process_group()\n",
    "\n",
    "\n",
    "def create_logger(logging_dir):\n",
    "    \"\"\"\n",
    "    Create a logger that writes to a log file and stdout.\n",
    "    \"\"\"\n",
    "    if dist.get_rank() == 0:  # real logger\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format='[\\033[34m%(asctime)s\\033[0m] %(message)s',\n",
    "            datefmt='%Y-%m-%d %H:%M:%S',\n",
    "            handlers=[logging.StreamHandler(), logging.FileHandler(f\"{logging_dir}/log.txt\")]\n",
    "        )\n",
    "        logger = logging.getLogger(__name__)\n",
    "    else:  # dummy logger (does nothing)\n",
    "        logger = logging.getLogger(__name__)\n",
    "        logger.addHandler(logging.NullHandler())\n",
    "    return logger\n",
    "\n",
    "\n",
    "def center_crop_arr(pil_image, image_size):\n",
    "    \"\"\"\n",
    "    Center cropping implementation from ADM.\n",
    "    https://github.com/openai/guided-diffusion/blob/8fb3ad9197f16bbc40620447b2742e13458d2831/guided_diffusion/image_datasets.py#L126\n",
    "    \"\"\"\n",
    "    while min(*pil_image.size) >= 2 * image_size:\n",
    "        pil_image = pil_image.resize(\n",
    "            tuple(x // 2 for x in pil_image.size), resample=Image.BOX\n",
    "        )\n",
    "\n",
    "    scale = image_size / min(*pil_image.size)\n",
    "    pil_image = pil_image.resize(\n",
    "        tuple(round(x * scale) for x in pil_image.size), resample=Image.BICUBIC\n",
    "    )\n",
    "\n",
    "    arr = np.array(pil_image)\n",
    "    crop_y = (arr.shape[0] - image_size) // 2\n",
    "    crop_x = (arr.shape[1] - image_size) // 2\n",
    "    return Image.fromarray(arr[crop_y: crop_y + image_size, crop_x: crop_x + image_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d51b5a33-5e3f-4aae-ac10-f22fd38e290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DiT(input_size=32, in_channels=3, depth=12, hidden_size=384, patch_size=4, num_heads=6) # small DiT\n",
    "model = model.to(device)\n",
    "\n",
    "ema = deepcopy(model).to(device)\n",
    "requires_grad(ema, False)\n",
    "\n",
    "# model = DDP(model.to(device), device_ids=[rank])\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "diffusion = create_diffusion(timestep_respacing=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3c3bb41-a230-4f2f-ae55-a682e624cda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "train_steps = 0\n",
    "log_steps = 0\n",
    "running_loss = 0\n",
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c33fb79-9249-4a93-b3e5-58db73cde75b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02e356d2e5844fc91ca76e13e719b77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 64, 384])\n",
      "tensor([ 3.2659e-01,  3.2876e-01, -5.1314e-01, -5.8167e-01,  7.7538e-02,\n",
      "         4.1261e-02, -5.2873e-01, -7.2869e-02, -1.0860e+00, -8.6710e-01,\n",
      "        -6.7228e-01,  1.1346e-01, -7.4606e-01, -1.2782e-01, -2.9290e-01,\n",
      "        -2.3251e-01, -4.2101e-01,  1.5367e-01,  3.1654e-02,  9.9865e-02,\n",
      "         3.8057e-01,  4.9915e-01,  4.7237e-01, -2.5264e-01,  3.5118e-01,\n",
      "        -2.3470e-01, -8.9649e-01,  4.2755e-01, -1.3740e-01,  2.6586e-01,\n",
      "         5.3159e-01, -4.5634e-01,  4.3722e-01, -2.5280e-01, -1.9763e-01,\n",
      "         1.6295e-01,  3.7256e-01,  5.9755e-01, -1.0493e-01,  1.4641e-01,\n",
      "        -2.8329e-01,  7.3738e-01, -8.7441e-01,  7.3340e-01, -3.1338e-01,\n",
      "        -8.9843e-02, -3.4737e-01,  2.5145e-01, -8.2669e-02, -3.2587e-01,\n",
      "         4.1496e-01,  8.0258e-01,  2.5291e-01,  8.7323e-01, -6.9772e-01,\n",
      "        -4.0320e-01, -2.6810e-01,  2.1619e-02,  3.3660e-02, -8.4476e-02,\n",
      "        -4.9327e-01,  3.4961e-01, -4.1084e-01,  6.2731e-01,  1.2510e-02,\n",
      "         4.4535e-01,  2.3248e-01, -5.9709e-01,  1.5367e-01, -4.0449e-01,\n",
      "         1.0245e+00, -4.4068e-01,  3.3763e-01,  1.8554e-01,  3.2156e-01,\n",
      "         5.5331e-01, -7.1027e-02, -1.0510e-01,  1.7347e-01, -2.0558e-01,\n",
      "         1.2062e+00, -7.6537e-02,  5.5443e-02,  2.0650e-01,  6.3556e-01,\n",
      "        -2.7894e-01,  1.8605e-01,  6.4795e-01,  8.2808e-01,  8.6122e-01,\n",
      "        -2.6821e-01,  1.4213e-03,  1.2998e-01,  4.9102e-01,  5.6833e-02,\n",
      "        -2.9150e-02,  1.0235e+00,  3.6280e-01,  7.1257e-01,  1.0293e+00,\n",
      "         6.5495e-01,  2.2065e+00,  7.3626e-01,  9.1205e-01,  1.1285e+00,\n",
      "         1.2190e+00,  7.2591e-01,  1.0231e+00,  8.8068e-01,  7.6258e-01,\n",
      "         9.6436e-01,  3.9802e-01,  1.8409e+00,  1.0751e+00,  6.8932e-01,\n",
      "         1.8305e+00,  3.0209e-01,  8.2100e-01,  6.0789e-01,  7.6223e-01,\n",
      "         8.2209e-01,  5.9559e-01,  1.5050e+00,  2.0019e+00,  1.3749e+00,\n",
      "         1.0894e+00,  8.9926e-01,  9.7786e-01,  7.9668e-01,  8.6348e-01,\n",
      "         1.1713e+00,  1.5633e+00,  9.4216e-01,  1.0715e+00,  1.3972e+00,\n",
      "         8.4036e-01,  1.3267e+00,  8.5630e-01,  1.5626e+00,  1.3019e+00,\n",
      "         9.1211e-01,  3.7839e-01,  1.0237e+00,  1.2778e+00,  1.0062e+00,\n",
      "         9.4650e-01,  1.7901e+00,  6.2054e-01,  1.4255e+00,  1.1475e+00,\n",
      "         1.3616e+00,  1.3820e+00,  9.8157e-01,  1.3229e+00,  5.9833e-01,\n",
      "         8.3729e-01,  1.0687e+00,  6.6041e-01,  6.2848e-01,  1.1810e+00,\n",
      "         1.2782e+00,  4.3767e-01,  3.8442e-01,  1.2726e+00,  5.6232e-01,\n",
      "         3.3632e-01,  3.1044e-01,  1.6667e+00,  8.6379e-01,  1.6963e+00,\n",
      "         7.1764e-01, -5.2086e-01,  1.1931e+00,  1.0946e+00,  1.1470e+00,\n",
      "         2.8665e-01,  1.3990e+00,  4.0456e-01,  1.0797e+00,  7.2981e-01,\n",
      "         1.0604e+00,  2.9777e-01,  1.2083e+00,  3.0522e-01,  9.8227e-01,\n",
      "         2.4620e+00,  1.4917e+00,  1.5521e+00,  3.7457e-01, -6.9714e-02,\n",
      "         1.4400e+00,  1.0158e+00,  1.0953e-01, -6.4701e-01,  8.9300e-02,\n",
      "        -1.3022e-01, -1.8805e-01,  8.0699e-01, -2.6049e-01,  2.0539e-01,\n",
      "         1.4772e-01, -4.7839e-01,  1.7352e-01,  1.8844e-01, -1.8028e-01,\n",
      "        -3.7549e-01,  1.8389e-01,  6.7136e-02,  2.7414e-01, -2.3310e-01,\n",
      "        -6.3399e-02, -7.6844e-02, -3.6744e-01,  1.2615e-01, -1.0584e+00,\n",
      "        -4.0660e-01,  1.8364e-01,  7.8207e-02,  8.3891e-01, -7.0319e-01,\n",
      "         1.0175e+00,  1.4232e-01, -1.6576e-01,  7.2348e-01, -5.6265e-02,\n",
      "         1.4320e-01,  3.3953e-01, -7.5196e-02,  5.1230e-01,  1.0367e-03,\n",
      "        -6.4002e-02,  9.4157e-01, -4.9265e-01,  5.4430e-02, -4.5420e-01,\n",
      "         5.5750e-01,  3.2558e-02,  1.0861e+00,  2.7139e-01,  2.2580e-01,\n",
      "        -4.8966e-01, -3.0736e-01,  3.7733e-01, -5.2976e-01, -1.1482e-01,\n",
      "         3.3311e-01, -8.3664e-01,  6.8749e-01,  3.4128e-01, -4.1458e-01,\n",
      "         4.7415e-03, -4.9135e-01, -5.9375e-01, -3.6857e-01, -4.8718e-01,\n",
      "        -6.8435e-01, -2.6329e-01,  2.2692e-01,  4.7185e-01,  2.0304e-01,\n",
      "         5.4393e-02,  7.2087e-01, -2.5038e-01, -2.5307e-01, -1.0606e-01,\n",
      "         3.1621e-01,  1.7490e-01,  8.1904e-02, -2.5210e-01,  1.1260e+00,\n",
      "         2.5207e-01, -6.2236e-01, -4.5867e-01, -3.2409e-01, -1.5226e-01,\n",
      "        -8.8210e-02, -5.6073e-01, -1.6206e-01, -8.9140e-01,  3.5753e-02,\n",
      "        -2.6352e-01, -7.2181e-01,  1.0585e-01,  6.4925e-02, -3.4302e-01,\n",
      "         5.2223e-01, -1.2956e-01,  6.3123e-02,  1.3382e+00, -2.4706e-01,\n",
      "         1.5514e+00,  1.3814e+00,  1.0454e+00,  4.8194e-01,  1.9682e-01,\n",
      "         1.2897e+00,  1.2051e+00,  1.5182e+00,  1.3971e+00,  1.4446e+00,\n",
      "         1.2171e+00,  1.2764e+00,  1.1423e+00,  4.7603e-01,  1.8744e+00,\n",
      "        -3.3176e-02,  1.4900e+00,  1.5647e+00,  9.3582e-01,  1.2715e+00,\n",
      "         1.5504e+00,  1.3372e+00,  1.4191e+00,  9.3490e-01,  1.4759e+00,\n",
      "         1.4660e+00,  1.1106e+00,  5.2698e-01,  7.5226e-01,  1.0931e+00,\n",
      "         6.0240e-01,  1.4672e+00,  1.2112e+00,  7.4264e-01,  1.1034e+00,\n",
      "         4.2606e-01,  1.9740e+00,  1.2095e+00,  7.1165e-01,  7.9401e-01,\n",
      "         8.2967e-01,  6.4967e-01,  1.7671e+00,  1.0206e+00, -4.0360e-01,\n",
      "         1.1699e+00,  2.7467e-01,  1.2409e+00,  2.8640e-01,  1.6298e+00,\n",
      "         1.2686e+00,  9.1767e-01,  8.6244e-01,  8.1006e-01,  1.7330e+00,\n",
      "         2.5207e-01,  6.4699e-01,  1.9785e-01,  1.4620e+00,  2.0204e+00,\n",
      "         1.4885e+00,  1.6314e+00,  1.0949e+00,  8.0020e-01,  3.5589e-01,\n",
      "         8.7131e-01,  8.2074e-01,  2.1860e+00,  1.2217e+00,  2.2091e+00,\n",
      "         2.0267e-03,  1.4304e+00,  1.5412e+00,  1.8645e+00,  1.7742e+00,\n",
      "         7.1875e-02,  1.2728e-01,  1.1458e+00,  1.5266e+00,  1.8144e+00,\n",
      "         3.8530e-01,  1.2696e+00,  9.2518e-01,  8.3217e-01,  7.9843e-01,\n",
      "         1.8003e+00,  1.0569e+00,  5.5065e-01,  7.4053e-01,  7.8921e-01,\n",
      "         1.5137e+00,  2.5240e-01,  8.5757e-01,  1.2443e+00], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "y shape: torch.Size([4, 384])\n",
      "c shape: torch.Size([4, 384])\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    for i, (x, y) in tqdm(enumerate(train_loader)):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        t = torch.randint(0, diffusion.num_timesteps, (x.shape[0],), device=device)\n",
    "\n",
    "        model_kwargs = dict(y=y)\n",
    "        loss_dict = diffusion.training_losses(model, x, t, model_kwargs)\n",
    "        loss = loss_dict[\"loss\"].mean()\n",
    "\n",
    "        break\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        #update_ema(ema, model.module)\n",
    "        update_ema(ema, model)\n",
    "\n",
    "        # Log loss values:\n",
    "        losses.append(loss.item())\n",
    "        running_loss += loss.item()\n",
    "        log_steps += 1\n",
    "        train_steps += 1\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Save the model\n",
    "torch.save(model.state_dict(), \"dit_cifar10.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96abe07b-0586-4af3-8bb1-c639907075e4",
   "metadata": {},
   "source": [
    "After patchifying, the shape of x is (B, T, D). B = batch size, T = sequence length, and D = hidden dimension. T = (input_size/patch_size)^2. For example, T = (32/4)^2 = 64 for input size 32 and patch size 4. D tells us how long the embedding vector is. Here, it's prespecified to be 384, so one channel just contains 384 numbers. The shape of y is (B, E), where B = batch size and E = hidden dimension. Thus, a modified input would need to adhere to these dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f796ae69-3d6a-4626-a6e2-47dc34442579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9f56214-b134-4445-8e08-eec9e1e16497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[] [] []\n"
     ]
    }
   ],
   "source": [
    "print(losses[1:10], losses[1000:1010], losses[3000:3010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9828465c-8a1b-429a-b942-6b0adc8bfbb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.16116527 0.28793052 0.43964188 ... 0.730531   0.78853008 0.8041579 ]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create an array of floats and indexes 0 to 3125\n",
    "x = np.arange(3125)  # Indexes from 0 to 3125\n",
    "y = np.random.rand(3126)  # Array of random floats\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3818f4b6-c04c-4232-bc36-87f93b12ba88",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (3125,) and (0,)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot the data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[0;32m----> 3\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlosses\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlot of Random Floats with Indexes 0 to 3125\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/module-jupyter-gpu/2.0-cuda-11-8/lib/python3.11/site-packages/matplotlib/pyplot.py:3578\u001b[0m, in \u001b[0;36mplot\u001b[0;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3570\u001b[0m \u001b[38;5;129m@_copy_docstring_and_deprecators\u001b[39m(Axes\u001b[38;5;241m.\u001b[39mplot)\n\u001b[1;32m   3571\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot\u001b[39m(\n\u001b[1;32m   3572\u001b[0m     \u001b[38;5;241m*\u001b[39margs: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m ArrayLike \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3576\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3577\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Line2D]:\n\u001b[0;32m-> 3578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgca\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mplot\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3579\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3580\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscalex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscalex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3581\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscaley\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscaley\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3582\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3583\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3584\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/module-jupyter-gpu/2.0-cuda-11-8/lib/python3.11/site-packages/matplotlib/axes/_axes.py:1721\u001b[0m, in \u001b[0;36mAxes.plot\u001b[0;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1478\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1479\u001b[0m \u001b[38;5;124;03mPlot y versus x as lines and/or markers.\u001b[39;00m\n\u001b[1;32m   1480\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1718\u001b[0m \u001b[38;5;124;03m(``'green'``) or hex strings (``'#008000'``).\u001b[39;00m\n\u001b[1;32m   1719\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1720\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m cbook\u001b[38;5;241m.\u001b[39mnormalize_kwargs(kwargs, mlines\u001b[38;5;241m.\u001b[39mLine2D)\n\u001b[0;32m-> 1721\u001b[0m lines \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_lines(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39mdata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)]\n\u001b[1;32m   1722\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m lines:\n\u001b[1;32m   1723\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madd_line(line)\n",
      "File \u001b[0;32m/opt/module-jupyter-gpu/2.0-cuda-11-8/lib/python3.11/site-packages/matplotlib/axes/_base.py:303\u001b[0m, in \u001b[0;36m_process_plot_var_args.__call__\u001b[0;34m(self, axes, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     this \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    302\u001b[0m     args \u001b[38;5;241m=\u001b[39m args[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m--> 303\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_plot_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mambiguous_fmt_datakey\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/module-jupyter-gpu/2.0-cuda-11-8/lib/python3.11/site-packages/matplotlib/axes/_base.py:499\u001b[0m, in \u001b[0;36m_process_plot_var_args._plot_args\u001b[0;34m(self, axes, tup, kwargs, return_kwargs, ambiguous_fmt_datakey)\u001b[0m\n\u001b[1;32m    496\u001b[0m     axes\u001b[38;5;241m.\u001b[39myaxis\u001b[38;5;241m.\u001b[39mupdate_units(y)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must have same first dimension, but \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    500\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhave shapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m y\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m    502\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y can be no greater than 2D, but have \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    503\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshapes \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (3125,) and (0,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA0UAAAH/CAYAAACYSXaPAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAgqUlEQVR4nO3df2zV9b348VehtNXttoswaxHs6q5ONjJ2aQOjrll0WgOGG2620MXFqheTNdsuF3rdBrLoIEuau5uZe52CWwTNEvQ2+Cv+0Tmamzt+CDcZTbssQrZFmIXZSlqzFnUrAp/7h1/6vV2Lcg5tsbwfj+T8cd683+e8j3nb8ORzTk9BlmVZAAAAJGraxd4AAADAxSSKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKTlHEW7d++O5cuXx+zZs6OgoCBeeOGFD1yza9euqK6ujpKSkrj22mvjsccey2evAAAA4y7nKHr77bdjwYIF8cgjj5zX/CNHjsSyZcuirq4uOjs74/7774/Vq1fHs88+m/NmAQAAxltBlmVZ3osLCuL555+PFStWnHPOd7/73XjxxRfj0KFDw2NNTU3x61//Ovbv35/vUwMAAIyLwol+gv3790d9ff2Isdtuuy22bt0a7777bsyYMWPUmqGhoRgaGhq+f+bMmXjzzTdj5syZUVBQMNFbBgAAPqSyLIsTJ07E7NmzY9q08fkVCRMeRb29vVFeXj5irLy8PE6dOhV9fX1RUVExak1LS0ts3LhxorcGAABMUUePHo05c+aMy2NNeBRFxKirO2ffsXeuqz7r16+P5ubm4fsDAwNxzTXXxNGjR6O0tHTiNgoAAHyoDQ4Oxty5c+Nv/uZvxu0xJzyKrrrqqujt7R0xdvz48SgsLIyZM2eOuaa4uDiKi4tHjZeWlooiAABgXD9WM+HfU7RkyZJob28fMbZz586oqakZ8/NEAAAAkynnKHrrrbeiq6srurq6IuK9X7nd1dUV3d3dEfHeW98aGxuH5zc1NcVrr70Wzc3NcejQodi2bVts3bo17rvvvvF5BQAAABcg57fPHThwIG666abh+2c/+3PXXXfFk08+GT09PcOBFBFRVVUVbW1tsXbt2nj00Udj9uzZ8fDDD8eXv/zlcdg+AADAhbmg7ymaLIODg1FWVhYDAwM+UwQAAAmbiDaY8M8UAQAAfJiJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICk5RVFmzdvjqqqqigpKYnq6urYs2fP+87fvn17LFiwIC6//PKoqKiIe+65J/r7+/PaMAAAwHjKOYpaW1tjzZo1sWHDhujs7Iy6urpYunRpdHd3jzl/79690djYGKtWrYpXXnklduzYEb/61a/i3nvvveDNAwAAXKico+ihhx6KVatWxb333hvz5s2Lf//3f4+5c+fGli1bxpz/P//zP/GJT3wiVq9eHVVVVfGFL3whvv71r8eBAwcuePMAAAAXKqcoOnnyZHR0dER9ff2I8fr6+ti3b9+Ya2pra+PYsWPR1tYWWZbFG2+8Ec8880zcfvvt+e8aAABgnOQURX19fXH69OkoLy8fMV5eXh69vb1jrqmtrY3t27dHQ0NDFBUVxVVXXRUf+9jH4sc//vE5n2doaCgGBwdH3AAAACZCXr9ooaCgYMT9LMtGjZ118ODBWL16dTzwwAPR0dERL730Uhw5ciSamprO+fgtLS1RVlY2fJs7d24+2wQAAPhABVmWZec7+eTJk3H55ZfHjh074h/+4R+Gx//5n/85urq6YteuXaPW3HnnnfGXv/wlduzYMTy2d+/eqKuri9dffz0qKipGrRkaGoqhoaHh+4ODgzF37twYGBiI0tLS835xAADApWVwcDDKysrGtQ1yulJUVFQU1dXV0d7ePmK8vb09amtrx1zzzjvvxLRpI59m+vTpEfHeFaaxFBcXR2lp6YgbAADARMj57XPNzc3x+OOPx7Zt2+LQoUOxdu3a6O7uHn473Pr166OxsXF4/vLly+O5556LLVu2xOHDh+Pll1+O1atXx6JFi2L27Nnj90oAAADyUJjrgoaGhujv749NmzZFT09PzJ8/P9ra2qKysjIiInp6ekZ8Z9Hdd98dJ06ciEceeST+5V/+JT72sY/FzTffHP/6r/86fq8CAAAgTzl9puhimYj3DQIAAFPPRf9MEQAAwKVGFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJyyuKNm/eHFVVVVFSUhLV1dWxZ8+e950/NDQUGzZsiMrKyiguLo5PfvKTsW3btrw2DAAAMJ4Kc13Q2toaa9asic2bN8eNN94YP/nJT2Lp0qVx8ODBuOaaa8Zcs3LlynjjjTdi69at8bd/+7dx/PjxOHXq1AVvHgAA4EIVZFmW5bJg8eLFsXDhwtiyZcvw2Lx582LFihXR0tIyav5LL70UX/3qV+Pw4cNxxRVX5LXJwcHBKCsri4GBgSgtLc3rMQAAgKlvItogp7fPnTx5Mjo6OqK+vn7EeH19fezbt2/MNS+++GLU1NTED3/4w7j66qvj+uuvj/vuuy/+/Oc/n/N5hoaGYnBwcMQNAABgIuT09rm+vr44ffp0lJeXjxgvLy+P3t7eMdccPnw49u7dGyUlJfH8889HX19ffOMb34g333zznJ8ramlpiY0bN+ayNQAAgLzk9YsWCgoKRtzPsmzU2FlnzpyJgoKC2L59eyxatCiWLVsWDz30UDz55JPnvFq0fv36GBgYGL4dPXo0n20CAAB8oJyuFM2aNSumT58+6qrQ8ePHR109OquioiKuvvrqKCsrGx6bN29eZFkWx44di+uuu27UmuLi4iguLs5lawAAAHnJ6UpRUVFRVFdXR3t7+4jx9vb2qK2tHXPNjTfeGK+//nq89dZbw2O/+93vYtq0aTFnzpw8tgwAADB+cn77XHNzczz++OOxbdu2OHToUKxduza6u7ujqakpIt5761tjY+Pw/DvuuCNmzpwZ99xzTxw8eDB2794d3/72t+Mf//Ef47LLLhu/VwIAAJCHnL+nqKGhIfr7+2PTpk3R09MT8+fPj7a2tqisrIyIiJ6enuju7h6e/9GPfjTa29vjn/7pn6KmpiZmzpwZK1eujB/84Afj9yoAAADylPP3FF0MvqcIAACI+BB8TxEAAMClRhQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJC2vKNq8eXNUVVVFSUlJVFdXx549e85r3csvvxyFhYXxuc99Lp+nBQAAGHc5R1Fra2usWbMmNmzYEJ2dnVFXVxdLly6N7u7u9103MDAQjY2N8aUvfSnvzQIAAIy3gizLslwWLF68OBYuXBhbtmwZHps3b16sWLEiWlpazrnuq1/9alx33XUxffr0eOGFF6Krq+u8n3NwcDDKyspiYGAgSktLc9kuAABwCZmINsjpStHJkyejo6Mj6uvrR4zX19fHvn37zrnuiSeeiFdffTUefPDB83qeoaGhGBwcHHEDAACYCDlFUV9fX5w+fTrKy8tHjJeXl0dvb++Ya37/+9/HunXrYvv27VFYWHhez9PS0hJlZWXDt7lz5+ayTQAAgPOW1y9aKCgoGHE/y7JRYxERp0+fjjvuuCM2btwY119//Xk//vr162NgYGD4dvTo0Xy2CQAA8IHO79LN/zNr1qyYPn36qKtCx48fH3X1KCLixIkTceDAgejs7IxvfetbERFx5syZyLIsCgsLY+fOnXHzzTePWldcXBzFxcW5bA0AACAvOV0pKioqiurq6mhvbx8x3t7eHrW1taPml5aWxm9+85vo6uoavjU1NcWnPvWp6OrqisWLF1/Y7gEAAC5QTleKIiKam5vjzjvvjJqamliyZEn89Kc/je7u7mhqaoqI99769sc//jF+9rOfxbRp02L+/Pkj1l955ZVRUlIyahwAAOBiyDmKGhoaor+/PzZt2hQ9PT0xf/78aGtri8rKyoiI6Onp+cDvLAIAAPiwyPl7ii4G31MEAABEfAi+pwgAAOBSI4oAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApIkiAAAgaaIIAABImigCAACSJooAAICkiSIAACBpoggAAEiaKAIAAJImigAAgKSJIgAAIGmiCAAASJooAgAAkiaKAACApOUVRZs3b46qqqooKSmJ6urq2LNnzznnPvfcc3HrrbfGxz/+8SgtLY0lS5bEL37xi7w3DAAAMJ5yjqLW1tZYs2ZNbNiwITo7O6Ouri6WLl0a3d3dY87fvXt33HrrrdHW1hYdHR1x0003xfLly6Ozs/OCNw8AAHChCrIsy3JZsHjx4li4cGFs2bJleGzevHmxYsWKaGlpOa/H+MxnPhMNDQ3xwAMPnNf8wcHBKCsri4GBgSgtLc1luwAAwCVkItogpytFJ0+ejI6Ojqivrx8xXl9fH/v27Tuvxzhz5kycOHEirrjiinPOGRoaisHBwRE3AACAiZBTFPX19cXp06ejvLx8xHh5eXn09vae12P86Ec/irfffjtWrlx5zjktLS1RVlY2fJs7d24u2wQAADhvef2ihYKCghH3sywbNTaWp59+Or7//e9Ha2trXHnlleect379+hgYGBi+HT16NJ9tAgAAfKDCXCbPmjUrpk+fPuqq0PHjx0ddPfprra2tsWrVqtixY0fccsst7zu3uLg4iouLc9kaAABAXnK6UlRUVBTV1dXR3t4+Yry9vT1qa2vPue7pp5+Ou+++O5566qm4/fbb89spAADABMjpSlFERHNzc9x5551RU1MTS5YsiZ/+9KfR3d0dTU1NEfHeW9/++Mc/xs9+9rOIeC+IGhsb4z/+4z/i85///PBVpssuuyzKysrG8aUAAADkLucoamhoiP7+/ti0aVP09PTE/Pnzo62tLSorKyMioqenZ8R3Fv3kJz+JU6dOxTe/+c345je/OTx+1113xZNPPnnhrwAAAOAC5Pw9RReD7ykCAAAiPgTfUwQAAHCpEUUAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkDRRBAAAJE0UAQAAScsrijZv3hxVVVVRUlIS1dXVsWfPnvedv2vXrqiuro6SkpK49tpr47HHHstrswAAAOMt5yhqbW2NNWvWxIYNG6KzszPq6upi6dKl0d3dPeb8I0eOxLJly6Kuri46Ozvj/vvvj9WrV8ezzz57wZsHAAC4UAVZlmW5LFi8eHEsXLgwtmzZMjw2b968WLFiRbS0tIya/93vfjdefPHFOHTo0PBYU1NT/PrXv479+/ef13MODg5GWVlZDAwMRGlpaS7bBQAALiET0QaFuUw+efJkdHR0xLp160aM19fXx759+8Zcs3///qivrx8xdtttt8XWrVvj3XffjRkzZoxaMzQ0FENDQ8P3BwYGIuK9/wAAAEC6zjZBjtd23ldOUdTX1xenT5+O8vLyEePl5eXR29s75pre3t4x5586dSr6+vqioqJi1JqWlpbYuHHjqPG5c+fmsl0AAOAS1d/fH2VlZePyWDlF0VkFBQUj7mdZNmrsg+aPNX7W+vXro7m5efj+n/70p6isrIzu7u5xe+EwlsHBwZg7d24cPXrUWzWZUM4ak8VZY7I4a0yWgYGBuOaaa+KKK64Yt8fMKYpmzZoV06dPH3VV6Pjx46OuBp111VVXjTm/sLAwZs6cOeaa4uLiKC4uHjVeVlbmfzImRWlpqbPGpHDWmCzOGpPFWWOyTJs2ft8ulNMjFRUVRXV1dbS3t48Yb29vj9ra2jHXLFmyZNT8nTt3Rk1NzZifJwIAAJhMOedVc3NzPP7447Ft27Y4dOhQrF27Nrq7u6OpqSki3nvrW2Nj4/D8pqameO2116K5uTkOHToU27Zti61bt8Z99903fq8CAAAgTzl/pqihoSH6+/tj06ZN0dPTE/Pnz4+2traorKyMiIienp4R31lUVVUVbW1tsXbt2nj00Udj9uzZ8fDDD8eXv/zl837O4uLiePDBB8d8Sx2MJ2eNyeKsMVmcNSaLs8ZkmYizlvP3FAEAAFxKxu/TSQAAAFOQKAIAAJImigAAgKSJIgAAIGkfmijavHlzVFVVRUlJSVRXV8eePXved/6uXbuiuro6SkpK4tprr43HHntsknbKVJfLWXvuuefi1ltvjY9//ONRWloaS5YsiV/84heTuFumslx/rp318ssvR2FhYXzuc5+b2A1yycj1rA0NDcWGDRuisrIyiouL45Of/GRs27ZtknbLVJbrWdu+fXssWLAgLr/88qioqIh77rkn+vv7J2m3TEW7d++O5cuXx+zZs6OgoCBeeOGFD1wzHl3woYii1tbWWLNmTWzYsCE6Ozujrq4uli5dOuJXe/9fR44ciWXLlkVdXV10dnbG/fffH6tXr45nn312knfOVJPrWdu9e3fceuut0dbWFh0dHXHTTTfF8uXLo7Ozc5J3zlST61k7a2BgIBobG+NLX/rSJO2UqS6fs7Zy5cr4r//6r9i6dWv89re/jaeffjpuuOGGSdw1U1GuZ23v3r3R2NgYq1atildeeSV27NgRv/rVr+Lee++d5J0zlbz99tuxYMGCeOSRR85r/rh1QfYhsGjRoqypqWnE2A033JCtW7duzPnf+c53shtuuGHE2Ne//vXs85///ITtkUtDrmdtLJ/+9KezjRs3jvfWuMTke9YaGhqy733ve9mDDz6YLViwYAJ3yKUi17P285//PCsrK8v6+/snY3tcQnI9a//2b/+WXXvttSPGHn744WzOnDkTtkcuLRGRPf/88+87Z7y64KJfKTp58mR0dHREfX39iPH6+vrYt2/fmGv2798/av5tt90WBw4ciHfffXfC9srUls9Z+2tnzpyJEydOxBVXXDERW+QSke9Ze+KJJ+LVV1+NBx98cKK3yCUin7P24osvRk1NTfzwhz+Mq6++Oq6//vq477774s9//vNkbJkpKp+zVltbG8eOHYu2trbIsizeeOONeOaZZ+L222+fjC2TiPHqgsLx3liu+vr64vTp01FeXj5ivLy8PHp7e8dc09vbO+b8U6dORV9fX1RUVEzYfpm68jlrf+1HP/pRvP3227Fy5cqJ2CKXiHzO2u9///tYt25d7NmzJwoLL/qPZqaIfM7a4cOHY+/evVFSUhLPP/989PX1xTe+8Y148803fa6Ic8rnrNXW1sb27dujoaEh/vKXv8SpU6fi7//+7+PHP/7xZGyZRIxXF1z0K0VnFRQUjLifZdmosQ+aP9Y4/LVcz9pZTz/9dHz/+9+P1tbWuPLKKydqe1xCzvesnT59Ou64447YuHFjXH/99ZO1PS4hufxcO3PmTBQUFMT27dtj0aJFsWzZsnjooYfiySefdLWID5TLWTt48GCsXr06Hnjggejo6IiXXnopjhw5Ek1NTZOxVRIyHl1w0f85ctasWTF9+vRR/8pw/PjxUdV31lVXXTXm/MLCwpg5c+aE7ZWpLZ+zdlZra2usWrUqduzYEbfccstEbpNLQK5n7cSJE3HgwIHo7OyMb33rWxHx3l9csyyLwsLC2LlzZ9x8882Tsnemlnx+rlVUVMTVV18dZWVlw2Pz5s2LLMvi2LFjcd11103onpma8jlrLS0tceONN8a3v/3tiIj47Gc/Gx/5yEeirq4ufvCDH3hnD+NivLrgol8pKioqiurq6mhvbx8x3t7eHrW1tWOuWbJkyaj5O3fujJqampgxY8aE7ZWpLZ+zFvHeFaK77747nnrqKe+D5rzketZKS0vjN7/5TXR1dQ3fmpqa4lOf+lR0dXXF4sWLJ2vrTDH5/Fy78cYb4/XXX4+33npreOx3v/tdTJs2LebMmTOh+2XqyuesvfPOOzFt2si/ak6fPj0i/v+/5MOFGrcuyOnXMkyQ//zP/8xmzJiRbd26NTt48GC2Zs2a7CMf+Uj2hz/8IcuyLFu3bl125513Ds8/fPhwdvnll2dr167NDh48mG3dujWbMWNG9swzz1ysl8AUketZe+qpp7LCwsLs0UcfzXp6eoZvf/rTny7WS2CKyPWs/TW/fY7zletZO3HiRDZnzpzsK1/5SvbKK69ku3btyq677rrs3nvvvVgvgSki17P2xBNPZIWFhdnmzZuzV199Ndu7d29WU1OTLVq06GK9BKaAEydOZJ2dnVlnZ2cWEdlDDz2UdXZ2Zq+99lqWZRPXBR+KKMqyLHv00UezysrKrKioKFu4cGG2a9eu4T+76667si9+8Ysj5v/yl7/M/u7v/i4rKirKPvGJT2RbtmyZ5B0zVeVy1r74xS9mETHqdtddd03+xplycv259n+JInKR61k7dOhQdsstt2SXXXZZNmfOnKy5uTl75513JnnXTEW5nrWHH344+/SnP51ddtllWUVFRfa1r30tO3bs2CTvmqnkv//7v9/3714T1QUFWeb6JQAAkK6L/pkiAACAi0kUAQAASRNFAABA0kQRAACQNFEEAAAkTRQBAABJE0UAAEDSRBEAAJA0UQQAACRNFAEAAEkTRQAAQNJEEQAAkLT/BTfQa128Jo/AAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the data\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, losses)\n",
    "plt.title(\"Plot of Random Floats with Indexes 0 to 3125\")\n",
    "plt.xlabel(\"Index\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d0f184-0c7a-4e97-813c-038de600f69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "# Labels to condition the model with (feel free to change):\n",
    "class_labels = [207, 360, 387, 974, 88, 979, 417, 279]\n",
    "\n",
    "# Create sampling noise:\n",
    "n = len(class_labels)\n",
    "z = torch.randn(n, 3, 32, 32, device=device)\n",
    "y = torch.tensor(class_labels, device=device)\n",
    "\n",
    "# Setup classifier-free guidance:\n",
    "z = torch.cat([z, z], 0)\n",
    "y_null = torch.tensor([1000] * n, device=device)\n",
    "y = torch.cat([y, y_null], 0)\n",
    "model_kwargs = dict(y=y, cfg_scale=1)\n",
    "\n",
    "# Sample images:\n",
    "samples = diffusion.p_sample_loop(\n",
    "    model.forward_with_cfg, z.shape, z, clip_denoised=False, model_kwargs=model_kwargs, progress=True, device=device\n",
    ")\n",
    "samples, _ = samples.chunk(2, dim=0)  # Remove null class samples\n",
    "#samples = vae.decode(samples / 0.18215).sample\n",
    "\n",
    "# Save and display images:\n",
    "save_image(samples, \"sample.png\", nrow=4, normalize=True, value_range=(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d637b79-7538-41e3-8db4-1a6ea6cf1b67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
