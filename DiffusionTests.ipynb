{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c83f3ba4-166c-4750-8c71-7d4dc8b8e08e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class DiffusionLM(nn.Module):\n",
    "    def __init__(self, model_name=\"gpt2\", num_diffusion_steps=200, noise_schedule=\"linear\", device = 'cuda'):\n",
    "        super(DiffusionLM, self).__init__()\n",
    "        \n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "        if self.tokenizer.mask_token is None:\n",
    "            self.tokenizer.add_special_tokens({'mask_token': '[MASK]'})\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
    "        \n",
    "        self.num_diffusion_steps = num_diffusion_steps\n",
    "        self.vocab_size = self.tokenizer.vocab_size + 2\n",
    "        \n",
    "        # Define the noise schedule (linear as default)\n",
    "        self.noise_schedule = self._create_noise_schedule(noise_schedule).to(device)\n",
    "\n",
    "    def _create_noise_schedule(self, schedule_type):\n",
    "        if schedule_type == \"linear\":\n",
    "            return torch.linspace(1e-4, 2e-2, self.num_diffusion_steps)\n",
    "        elif schedule_type == \"exponential\":\n",
    "            return torch.exp(torch.linspace(-4, 0, self.num_diffusion_steps))\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported noise schedule: {schedule_type}\")\n",
    "\n",
    "    def forward_diffusion(self, input_ids, t):\n",
    "        \"\"\"\n",
    "        Forward diffusion: Corrupt tokens by replacing a proportion with noise.\n",
    "        \"\"\"\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        noise_level = self.noise_schedule[t].to(input_ids.device)\n",
    "        mask = torch.rand(batch_size, seq_len, device=input_ids.device) < noise_level\n",
    "        noisy_input = input_ids.clone()\n",
    "        noisy_input[mask] = self.tokenizer.mask_token_id  # Replace tokens with [MASK]\n",
    "        return noisy_input, mask\n",
    "\n",
    "    def reverse_diffusion(self, noisy_input, t, attention_mask=None):\n",
    "        \"\"\"\n",
    "        Reverse diffusion: Predict the token distribution for denoising.\n",
    "        \"\"\"\n",
    "        outputs = self.model(input_ids=noisy_input, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        return logits\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, t):\n",
    "        \"\"\"\n",
    "        Full diffusion process: forward + reverse.\n",
    "        \"\"\"\n",
    "        noisy_input, mask = self.forward_diffusion(input_ids, t)\n",
    "        logits = self.reverse_diffusion(noisy_input, t, attention_mask)\n",
    "        return logits, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c5e6ce3d-8b8e-4f48-b541-5dfd1d34eb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_diffusion_lm(model, train_dataloader, num_epochs=5, lr=5e-5):\n",
    "    \"\"\"\n",
    "    Training loop for Diffusion-LM with tqdm for progress tracking.\n",
    "    \"\"\"\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #device = 'cpu'\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        #tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\"\n",
    "        # Initialize tqdm progress bar\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{num_epochs}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "\n",
    "            t = torch.randint(0, model.num_diffusion_steps - 1, (input_ids.size(0),)).to(device)\n",
    "            t = t.unsqueeze(1).expand(-1, input_ids.size(1))\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            logits, mask = model(input_ids, attention_mask, t)\n",
    "            target = input_ids.clone()\n",
    "            target[~mask] = -100  # Ignore uncorrupted tokens in loss calculation\n",
    "            \n",
    "            loss = loss_fn(logits.view(-1, model.vocab_size), target.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())  # Update tqdm bar with current loss\n",
    "\n",
    "        # End of epoch summary\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Average Loss: {total_loss / len(train_dataloader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cc203a8a-21da-4931-af4f-0cf52719a705",
   "metadata": {},
   "outputs": [],
   "source": [
    "class E2EDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for E2E data (input-output pairs).\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path, tokenizer, max_length=128, device = 'cuda'):\n",
    "        self.data = []\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "\n",
    "        # Read and parse the file\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                self.data.append(line.strip())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize each line as input and target\n",
    "        text = self.data[idx]\n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0).to(torch.device(self.device)),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0).to(torch.device(self.device))\n",
    "        }\n",
    "\n",
    "def load_e2e_data(data_dir, tokenizer_name=\"gpt2\", batch_size=16, max_length=128):\n",
    "    \"\"\"\n",
    "    Load train, validation, and test datasets from the E2E folder.\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "\n",
    "    if tokenizer.mask_token is None:\n",
    "        # If not, set the mask token (this could be any token you'd like to use, e.g., '[MASK]')\n",
    "        tokenizer.add_special_tokens({'mask_token': '[MASK]'})\n",
    "        \n",
    "\n",
    "    # Paths to the train, validation, and test files\n",
    "    train_path = os.path.join(data_dir, \"src1_train.txt\")\n",
    "    val_path = os.path.join(data_dir, \"src1_valid.txt\")\n",
    "    test_path = os.path.join(data_dir, \"src1_test.txt\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = E2EDataset(train_path, tokenizer, max_length=max_length)\n",
    "    val_dataset = E2EDataset(val_path, tokenizer, max_length=max_length)\n",
    "    test_dataset = E2EDataset(test_path, tokenizer, max_length=max_length)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "610a829d-f104-4e5a-8a65-3e60a61ada97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the E2E dataset folder\n",
    "data_dir = \"Diffusion-LM/datasets/e2e_data\"\n",
    "\n",
    "# Load the dataset\n",
    "train_dataloader, val_dataloader, test_dataloader = load_e2e_data(data_dir, tokenizer_name=\"gpt2\", batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1130b1cb-6aee-421c-bf6b-a197bc48282b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 5258/5258 [04:10<00:00, 20.98it/s, loss=nan]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Average Loss: nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = DiffusionLM(model_name=\"gpt2\", num_diffusion_steps=128, device = 'cuda')\n",
    "\n",
    "# Train the model\n",
    "train_diffusion_lm(model, train_dataloader, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b366621f-f71f-4517-977e-d89864ba1915",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'name The Fyne | Type : Chinese | customer rating : 5 | area : rivers centre | family : The Rice Boat||Loch Fyne is English food and the city centre near The Rice Boat . It customer rating are average . The'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    data_input = batch['input_ids']\n",
    "    data_att_mask = batch['attention_mask']\n",
    "\n",
    "    break\n",
    "\n",
    "t = torch.randint(0, model.num_diffusion_steps - 1, (data_input.size(0),))\n",
    "t = t.unsqueeze(1).expand(-1, data_input.size(1))\n",
    "\n",
    "model.eval()\n",
    "logits, mask = model(data_input, data_att_mask, t)\n",
    "\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Decode the predicted token ids to text\n",
    "generated_text = model.tokenizer.decode(predicted_ids[0], skip_special_tokens=True)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "52cbc791-4ae7-4cba-a220-408fa33eaba4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'name : Loch Fyne | food : English | customer rating : average | area : city centre | near : The Rice Boat||Loch Fyne serves English food in the city centre near The Rice Boat . The customer ratings are average .'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(data_input[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "191749da-696b-4bfc-b6b5-ab37e944249a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3672,  1058, 45264,   376, 39547,   930,  2057,  1058,  3594,   930,\n",
       "         6491,  7955,  1058,  2811,   930,  1989,  1058,  1748,  7372,   930,\n",
       "         1474,  1058,   383, 13823, 30828, 15886,    43,  5374,   376, 39547,\n",
       "         9179,  3594,  2057,   287,   262,  1748,  7372,  1474,   383, 13823,\n",
       "        30828,   764,   383,  6491, 10109,   389,  2811,   764, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d9fadc01-0ebb-4dde-b89d-dee7b35dd268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3672,   383,   376, 39547,   930,  5994,  1058,  3999,   930,  6491,\n",
       "         7955,  1058,   642,   930,  1989,  1058, 18180,  7372,   930,  1641,\n",
       "         1058,   383, 13823, 30828, 15886,    43,  5374,   376, 39547,   318,\n",
       "         3594,  2057,   290,   262,  1748,  7372,  1474,   383, 13823, 30828,\n",
       "          764,   632,  6491,  7955,   389,  2811,   764,   383, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257,\n",
       "        50257, 50257, 50257, 50257, 50257, 50257, 50257, 50257],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_ids[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
